\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{4}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Transformers}{4}{section.2}\protected@file@percent }
\newlabel{sec:transformer}{{2}{4}{Transformers}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Input Embedding Layer}{4}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Input embedding of sentence\relax }}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:input_embedding_mechanism}{{1}{5}{Input embedding of sentence\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces CBOW's model architecture\relax }}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:cbow}{{2}{6}{CBOW's model architecture\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Transformer Encoder}{6}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Transformer's Encoder architecture\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:transformer_encoder}{{3}{7}{Transformer's Encoder architecture\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Positional Embeddings}{8}{subsection.2.3}\protected@file@percent }
\newlabel{eq:transformer_positional_encoding}{{3}{8}{Positional Embeddings}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Encoding Layer}{8}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Multi-Headed Attention Layer architecture in detail.\relax }}{9}{figure.caption.5}\protected@file@percent }
\newlabel{fig:transformer_multi_headed_attention_layer}{{4}{9}{Multi-Headed Attention Layer architecture in detail.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Multi-Head Attention Layer}{9}{subsubsection.2.3.2}\protected@file@percent }
\newlabel{subsubsec:transformer_encoder_multi_head}{{2.3.2}{9}{Multi-Head Attention Layer}{subsubsection.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Linear Projection Layer architecture. As we can see, the resulting matrices $Q, K,$ and $V$ are all of the shape $n \times D$ because they are logically split among the heads.\relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{fig:multi_head_linear_projection_layer}{{5}{10}{Linear Projection Layer architecture. As we can see, the resulting matrices $Q, K,$ and $V$ are all of the shape $n \times D$ because they are logically split among the heads.\relax }{figure.caption.6}{}}
\newlabel{eq:multi_head_attention_linear_projection_single_head}{{5}{10}{Multi-Head Attention Layer}{equation.2.5}{}}
\newlabel{eq:transformer_self_attention_score}{{10}{11}{Multi-Head Attention Layer}{equation.2.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Normalization and Residual Connections}{11}{subsubsection.2.3.3}\protected@file@percent }
\newlabel{subsubsec:transformer_normalization_layer}{{2.3.3}{11}{Normalization and Residual Connections}{subsubsection.2.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}Feed-Forward Network}{11}{subsubsection.2.3.4}\protected@file@percent }
\newlabel{subsubsec:transformer_encoder_feed_forward}{{2.3.4}{11}{Feed-Forward Network}{subsubsection.2.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax }}{12}{figure.caption.7}\protected@file@percent }
\newlabel{fig:transformer_encoder_add_residual}{{6}{12}{\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Considerantions about Self-Attention}{12}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \relax }}{13}{figure.caption.8}\protected@file@percent }
\newlabel{fig:transformer_encoder_ffn}{{7}{13}{\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Output Encoding}{13}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Transformer Decoder}{13}{subsection.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Transformer's Decoder architecture\relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig:transformer_decoder_architecture}{{8}{14}{Transformer's Decoder architecture\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Masked Self-Attention mechanism simplified.\relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{fig:transformer_decoder_self_attention}{{9}{15}{Masked Self-Attention mechanism simplified.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Positional Embedding}{15}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Decoding Layer}{15}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Masked Self-Attention Mechanism}{15}{subsubsection.2.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.4}Encoder-Decoder Multi-Head Attention or Cross Attention}{15}{subsubsection.2.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Multi-head attention layer in the decoding layer. As we can see, the structure of the multi-head attention layer is exactly like the one described in the encoding layer section. The main difference stands in the input provided to it, since we are combining the encoder output and the decoder masked mulit-head layer output.\relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{fig:transformer_multi_headed_attention_layer_decoder}{{10}{16}{Multi-head attention layer in the decoding layer. As we can see, the structure of the multi-head attention layer is exactly like the one described in the encoding layer section. The main difference stands in the input provided to it, since we are combining the encoder output and the decoder masked mulit-head layer output.\relax }{figure.caption.11}{}}
\newlabel{eq:decoder_multi_head_attention_linear_projection_single_head}{{17}{16}{Encoder-Decoder Multi-Head Attention or Cross Attention}{equation.2.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.5}Feed-Forward Network}{16}{subsubsection.2.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.6}Linear Projection Layer and Softmax Activation Function}{17}{subsubsection.2.6.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Loss}{17}{subsection.2.7}\protected@file@percent }
\newlabel{subsec:transformer_loss}{{2.7}{17}{Loss}{subsection.2.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Vision Transformer (ViT)}{17}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Vision Transformer (ViT) original architecture.\relax }}{18}{figure.caption.12}\protected@file@percent }
\newlabel{fig:vit_architecture}{{11}{18}{Vision Transformer (ViT) original architecture.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Input Pre-processing}{18}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}ViT Encoder}{19}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Multi-head Attention Network (MAH or MSP)}{19}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Residual Connections and Norm Layer}{20}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Multi-Layer Perceptrons (MLP)}{20}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}ViT Classifier}{21}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Loss}{21}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Detection Transformer (DETR)}{21}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Detection Transformer (DETR) architecture.\relax }}{22}{figure.caption.13}\protected@file@percent }
\newlabel{fig:detr_architecture}{{12}{22}{Detection Transformer (DETR) architecture.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}CNN Backbone Module}{22}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Transformer Module}{22}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}DETR Encoder Module}{22}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Detailed DETR architecture.\relax }}{23}{figure.caption.14}\protected@file@percent }
\newlabel{fig:detr_architecture_2}{{13}{23}{Detailed DETR architecture.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}DETR Decoder Module}{23}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}DETR Prediction Module}{23}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces DETR Transformer module.\relax }}{24}{figure.caption.15}\protected@file@percent }
\newlabel{fig:detr_transformer_module}{{14}{24}{DETR Transformer module.\relax }{figure.caption.15}{}}
\newlabel{eq:detr_bbox_prediction}{{32}{25}{DETR Prediction Module}{equation.4.32}{}}
\newlabel{eq:detr_class_prediction}{{33}{25}{DETR Prediction Module}{equation.4.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Deformable DETR}{25}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Deformable DETR multi-scale feature maps. On the right side, we have the last three ResNet convolutional blocks; the intermediate output is then used to build the multi-scale feature maps on the left side.\relax }}{26}{figure.caption.16}\protected@file@percent }
\newlabel{fig:deformable_detr_multi_scale_feature_maps}{{15}{26}{Deformable DETR multi-scale feature maps. On the right side, we have the last three ResNet convolutional blocks; the intermediate output is then used to build the multi-scale feature maps on the left side.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Deformable Attention Mechanism}{26}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Features Maps Flattening}{26}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Deformable Attention Mechanism}{27}{subsection.5.2}\protected@file@percent }
\gdef \@abspage@last{27}
